[model]
########################################################################################################################
# Feature map settings
# fm_extent: size of point cloud crop used as input, larger crops require more memory
# voxel_size: .4 is default for 3DHDNet
# max_num_voxels: maximize the number of input voxels (speed)
# -> approx. 1-2 points per 0.1 m^3 (VoxelNet), ~30 per 0.1 m^2 (PointPillars), 96 for voxel_size=.4, 24 for voxel_size=.2
# min_points_per_voxel: discard voxels that contain fewer points
fm_extent = [[-10.0, 18.8], [-20.0, 20.0], [-2.0, 7.6]]     # [x_min, x_max], [y_min, y_max], [z_min, z_max]
voxel_size = [0.40, 0.40, 0.40]                             # x,y,z | PointPillars, VoxelNet: [0.20, 0.20, 9.6]
max_num_voxels = 0                                          # 0 for no filter, 70000, ...
max_points_per_voxel = 96                                   # maximum number of points allowed in a voxel
min_points_per_voxel = 3                                    # minimum is 1

# Automatic size adjustments
# target_shape_factor: set resolution of anchor grid to be of same size as the voxel grid
auto_generate_shapes = True     # auto-generates fm_shape_{hdpc,map} and target_shape from fm_extent and voxel_size
target_shape_factor = 1.0       # factor 0.5: output grid is half the size of the input grid

# Feature maps
# hdpc: high-density point cloud
fm_type_hdpc = "voxels"         # only option
z_encoding_hdpc = "global"      # only option

# fm_type_map: "voxels_lut" to generate the map representation as specified our article
# fm_type_map: None: no map representation is generated (only load map elements)
fm_type_map = None
map_fm_element_types = ['poles', 'signs', 'lights']

# element_size_factor: enlarges map elements for voxel matching (generation of map representation)
element_size_factor = .55                   # 0.5: keep size (equals factor 1.1 as specified in our article)
map_fm_fusion_type = "early"                # early (fuse map before backbone), late (fuse map after backbone)

join_stacked_signs_shapes = ['rectangle']
join_stacked_signs_width_threshold = 0.1    #  width threshold, if lower: vertically stacked signs are fused [meter]

########################################################################################################################
# Anchor generation
loss_type = "CombinedLoss"  # only option
target_type = "anchors"     # only option

# Association thresholds for measures regarding the overlap of map elements
pole_iosa_threshold = .05               # IosA (intersection over small area) threshold for poles (modeled as circles)
sign_distance_threshold = .2            # for line segment distance in xy plane | should be >= voxel_size/2
sign_z_foreground_threshold = .2        # > threshold -> match
sign_z_background_threshold = .1        # < threshold -> no match (between thresholds: don't care)
light_iosa_threshold = .05              # base plate of light bounding boxes are modeled as circles for simplicity
light_z_foreground_threshold = .2       # threshold for z overlap, > threshold -> match
light_z_background_threshold = .1       # < threshold -> no match (between thresholds: don't care)

# Anchor parameters
pole_default_diameter = 0.2             # default anchor diameter for poles [meter]
pole_z_stride = 1.0                     # default z_stride value for poles, only used for normalization purposes [meter]

sign_default_height = 0.65              # default sign height [meter]
sign_default_width = 0.65               # default sign width [meter]
sign_default_yaw = 0.0                  # default sign yaw [degree]
sign_z_min = 0.0                        # minimum z position of anchors
sign_z_max = 6.0                        # maximum z position of anchors
sign_z_stride = 0.6                     # stride between vertical anchor layers

light_default_height = 0.9              # default light height [meter]
light_default_width = 0.3               # default light size [meter]
light_default_yaw = 0.0                 # default light yaw [degree]
light_z_min = 1.0                       # minimum z position of anchors
light_z_max = 7.0                       # maximum z position of anchors
light_z_stride = 1.0                    # stride between vertical anchor layers

# set_anchor_layers_like_voxels: true -> automatically adjust {sign, light}_z_{min, max, stride}
# -> place one anchor layer for every voxel layer (necessary when using 3d_heads)
# classification_active: enables pole, sign, and light classification of 3DHDNet (only for object detection task)
set_anchor_layers_like_voxels = True    # voxels: input voxel size (LiDAR + map fm)
classification_active = False           # True: classification enabled (e.g., circle, rectangle, etc. for signs)
use_recommended_pole_classes = True     # True: use all available pole classes
use_recommended_sign_classes = True     # True: use all available sign classes
use_recommended_light_classes = True    # True: use all available light classes
pole_classes = []                       # alternatively: specify list of pole classes
sign_classes = []                       # alternatively: specify list of sign classes
light_classes = []                      # alternatively: specify list of light classes

########################################################################################################################
# Model architecture
# configured_element_types: defines element types to predict
configured_element_types = ['lights', 'poles', 'signs']
deviation_detection_model = False           # true for deviation detection DNN (MDD-M = stage 3)

# Encoder
# vfe: voxel feature extractor (encoder)
num_input_features = 4                      # x, y, z, i
vfe_class_name = "FeatureNet3DHD"           # used encoder: FeatureNet3DHD, PillarFeatureNet, VoxelFeatureExtractor
vfe_num_filters = [128, 256]                # encoder layers with channels | [64,] PointPillars, [32 128] VoxelNet
# Middle Layer
middle_class_name = "VoxelnetScatter"       # PointPillarsScatter, VoxelnetScatter, MiddleExtractor, MiddleExtractorExtended
middle_num_layers = [2, 1]                  # used for MiddleExtractorExtended (3D to BEV mapping, only VoxelNet)
middle_num_filters = [64, 64]               # used for MiddleExtractorExtended
middle_z_strides = [2, 2]                   # used for MiddleExtractorExtended
middle_num_filters_d1 = [64]                # used for MiddleExtractor
middle_num_filters_d2 = [64]                # used for MiddleExtractor
# Backbone
bb_layer_nums = [3, 5, 5]                   # number of convolutional layers in "Dn" blocks (see 3DHDNet paper)
bb_layer_strides = [1, 2, 2]                # stride used in "Dn" (downsample) blocks (2: half size)
bb_num_filters = [64, 128, 256]             # number of features used in each "Dn" block
bb_upsample_strides = [1, 2, 4]             # strides for "Up" blocks (2: double size)
bb_num_upsample_filters = [128, 128, 128]   # number of features in "Up" blocks
use_norm = True                             # use BatchNorm
use_3d_backbone = True                      # use 3D convolutions in backbone (instead of 2d), required for 3DHDNet
use_3d_heads = True                         # use 3D convolutions in output heads (only with 3D backbone)
head_num_filters = []                       # use additional layers for each output head

[train]
########################################################################################################################
# Dataset
# load level specifies the data loading procedure
# -> generated: samples (hdpc or map) are generated prior to training and only loaded (requires 2TB)
# -> online: samples are generated on the fly (requires approx. 50 GB RAM, 20% slower than 'generated')
load_lvl_hdpc = "online"    # generated, online
load_lvl_map = "online"
prefetch_pc_tiles = True       # speedup for online training

# Initialization
# init_mode: 'scratch' trains a new model, 'pretrained' resumes training
init_mode = "scratch"               # pretrained, scratch
experiment_name_pretrained = ""     # set the name of the pretrained net for which training should be resumed
start_iteration = 1000              # only for 'pretrained': set init value of iteration counter

# Training
batch_size = 2              # batch size
num_epochs = 8              # number of epochs to train

# Optimizer
optimizer = 'adam'          # only option
momentum = 0.0              # not used for adam
weight_decay = 0.0001
learning_rate = 0.0002

# Weights
weight_dict = {'element': .25, 'no_element': .75}   # weight factors for focal loss
use_class_specific_weights = False                  # specify weight of deviation classes for all element types
weight_dict_classes = {'VER': 1.0, 'INS': 1.0, 'DEL': 1.0, 'SUB': 1.0}

# Stop condition
iteration_stop_condition = False    # True if training is to be stopped after a number of iterations reached
num_iterations = 200000000          # max number of iterations to train (only if iteration_stop_condition is set)

# Sample filter
# -> set the number of elements required to exist in a crop to consider a sample as valid (training, validation, and test)
lights_per_sample_min = 0
poles_per_sample_min = 0
signs_per_sample_min = 0
any_elements_per_sample_min = 0     # disregards the type

# Augmentation
global_augmentation = True      # global augmentation (translation, rotation) of both point cloud and map
point_dropout = True            # randomly remove points
local_augmentation = False      # locally shift group of map elements
noise_per_point = False         # add noise (x,y,z) to each point

global_trans_x = 2.0            # global translation x [meter] in VRF (x points towards the vehicle's front)
global_trans_y = 0.2            # global translation y [meter]
global_trans_z = 0.2            # global translation z [meter]
global_yaw_uni = 20             # global rotation of point cloud and map[degree]
global_scaling = 0.05           # global scaling factor (0.05 -> scale between 1.05 and 0.95)
random_flip_prob = 0.5          # randomly flip y coordinates
equal_intensity_prob = 0.0      # probability of setting all points to the same intensity value (zero)

# Local augmentation settings
local_t_center = 0              # locally translate element groups [meter]
local_yaw_uni = 45              # locally rotate " [degree]
local_scaling = 0.05            # locally scale "
association_threshold = 1       # association distance while creating augmentation groups [meter]
association_consider_z = False  # consider the z-dimension while creating augmentation groups
group_augmentation = True       # False -> crop and augment individual elements (only if local augmentation is set)

# Point cloud augmentation
point_keep_prob = .8            # probability of keeping a point of the point cloud
noise_per_point_std = .03       # noise per point [meter]

# Deviation Detection
deviation_detection_task = False            # True: deviation detection task. False: object detection task
dd_stage = 0                                # method variant for deviation detection task. 1: MDD-SC, 2: MDD-MC, 3: MDD-M
deletion_prob = 0.2                         # training: deletion (DEL) probability (for deviation simulation)
insertion_prob = 0.2                        # training: insertion (INS) probability
substitution_prob = 0.1                     # training: substitution (SUB) probability
load_generated_deviations = True            # True: load json files generated prior to training (fixed deviation assignments)
generated_deviations_setting = '10-10-5'    # Specify which deviation setting (del-ins-sub probability) to load (inference)

# Ablation studies
occlusion_prob = 0.0                        # specify the occlusion setting to generate or to load
dd_point_density = 1.0                      # 1.0 == no point density modification, keep all points

# Experiment Tracking
# tracking_backend: sets the tracking backend, mlflow is recommended
tracking_backend = ['mlflow', 'tensorboard']  # specify tracking backend which to use for logging
mlflow_group_name = ''                        # used by mlflow to group experiments (if empty, will be named after configured element types)

# Training log
train_log_interval = 1000               # training iteration interval to log metrics and images

# Validation
# validation_mode: specifies when to run validation (after epoch, an interval of iterations, both, or not at all)
validation_mode = 'disabled'             # epoch, interval, both, disabled
validation_interval = 1000               # interval of iterations
num_validation_batches = 100             # number of validation batches to run when validating
validation_log_interval = 10             # iteration interval to log images

# Inference (NMS thresholds)
threshold_score = .25
threshold_association_distance = .3     # Euclidean distance threshold [meter] for poles and substitutions

# Save model
checkpoint_interval = 0                 # specify the number of iterations for model saving
epoch_checkpoint_interval = 10           # specify the number of epochs for model saving
early_stopping_active = False           # True for early stopping, tracks the validation loss
max_failed_improvements = 10            # Number of validation runs with an increased validation loss after which to stop training

inference_partitions = ['val', 'test']  # dataset partitions to load for inference
dataset_version = ''

[system]
########################################################################################################################
# System settings
experiment = 'test'             # experiment name
mlflow_experiment_id = None     # used by mlflow (id of experiment group)
mlflow_run_id = None            # used by mlflow (id of experiment)
# auto_configure_paths: set required paths according to system_paths.py, configure your system there
auto_configure_paths = True

# Training pipeline
# Uncomment for DDP (Distributed Data Parallel)
ddp = True                      # enable DPP using independent processes for each GPU
multi_gpu = False               # standard data parallelism (False if DDP is used instead)
use_batch_sync = True           # DDP: synchronize batches between GPUs
num_nodes = 1                   # DDP: number of nodes
rank = 0                        # DDP: ranking of host process
num_workers = 8                 # Number of threads working the input pipeline
num_gpus = 2                    # number of GPUs used for training
train_device = 'cuda:0'         # device for main process
use_amp = True                  # automatic mixed precision | model requires less VRAM if GPU is compatible

# Uncomment for debug mode
;ddp = False
;multi_gpu = False
;num_workers = 0
;train_device = 'cuda:0'    # cpu
;use_amp = False

# Uncomment for ordinary DP (DataParallel with multiple GPUs)
;ddp = False
;multi_gpu = True
;num_workers = 8
;train_device = 'cuda:0'
;train_device_ids = [0, 1]

